{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\"\", \"checkpoints_train/cifar10/24-05-2021--11-12-22/best.pt\", \"16\" ,\"512\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pre_processing', 'pre_processing_0', 'pre_processing_1', 'main_net', 'main_net_0', 'main_net_0_ops', 'main_net_0_ops_preproc_x', 'main_net_0_ops_preproc_x_op', 'main_net_0_ops_preproc_x_op_0', 'main_net_0_ops_preproc_x_op_1', 'main_net_0_ops_preproc_x_op_2', 'main_net_0_ops_preproc_x2', 'main_net_0_ops_preproc_x2_op', 'main_net_0_ops_preproc_x2_op_0', 'main_net_0_ops_preproc_x2_op_1', 'main_net_0_ops_preproc_x2_op_2', 'main_net_0_ops_(0, 2)', 'main_net_0_ops_(0, 2)_ops', 'main_net_0_ops_(0, 2)_ops_(0, 1)', 'main_net_0_ops_(0, 2)_ops_(0, 1)_op', 'main_net_0_ops_(0, 2)_ops_(0, 1)_op_0', 'main_net_0_ops_(0, 2)_ops_(0, 1)_op_1', 'main_net_0_ops_(0, 2)_ops_(0, 1)_op_2', 'main_net_0_ops_(0, 2)_ops_(0, 1)_op_3', 'main_net_0_ops_(0, 2)_ops_(0, 1)_op_4', 'main_net_0_ops_(0, 2)_ops_(0, 1)_op_5', 'main_net_0_ops_(0, 2)_ops_(0, 1)_op_6', 'main_net_0_ops_(0, 2)_ops_(0, 1)_op_7', 'main_net_0_ops_(0, 2)_ops_(0, 2)', 'main_net_0_ops_(0, 2)_ops_(0, 2)_op', 'main_net_0_ops_(0, 2)_ops_(0, 2)_op_0', 'main_net_0_ops_(0, 2)_ops_(0, 2)_op_1', 'main_net_0_ops_(0, 2)_ops_(0, 2)_op_2', 'main_net_0_ops_(0, 2)_ops_(0, 2)_op_3', 'main_net_0_ops_(0, 2)_ops_(1, 2)', 'main_net_0_ops_(0, 2)_ops_(1, 2)_op', 'main_net_0_ops_(0, 2)_ops_(1, 2)_op_0', 'main_net_0_ops_(0, 2)_ops_(1, 2)_op_1', 'main_net_0_ops_(0, 2)_ops_(1, 2)_op_2', 'main_net_0_ops_(0, 2)_ops_(1, 2)_op_3', 'main_net_0_ops_(1, 2)', 'main_net_0_ops_(1, 2)_ops', 'main_net_0_ops_(1, 2)_ops_(0, 1)', 'main_net_0_ops_(1, 2)_ops_(0, 1)_op', 'main_net_0_ops_(1, 2)_ops_(0, 2)', 'main_net_0_ops_(1, 2)_ops_(0, 2)_op', 'main_net_0_ops_(1, 2)_ops_(0, 2)_op_0', 'main_net_0_ops_(1, 2)_ops_(0, 2)_op_1', 'main_net_0_ops_(1, 2)_ops_(0, 2)_op_2', 'main_net_0_ops_(1, 2)_ops_(0, 2)_op_3', 'main_net_0_ops_(1, 2)_ops_(0, 2)_op_4', 'main_net_0_ops_(1, 2)_ops_(0, 2)_op_5', 'main_net_0_ops_(1, 2)_ops_(0, 2)_op_6', 'main_net_0_ops_(1, 2)_ops_(0, 2)_op_7', 'main_net_0_ops_(1, 2)_ops_(1, 2)', 'main_net_0_ops_(1, 2)_ops_(1, 2)_op', 'main_net_0_ops_(1, 2)_ops_(1, 2)_op_0', 'main_net_0_ops_(1, 2)_ops_(1, 2)_op_1', 'main_net_0_ops_(1, 2)_ops_(1, 2)_op_2', 'main_net_0_ops_(1, 2)_ops_(1, 2)_op_3', 'main_net_0_ops_(1, 2)_ops_(1, 2)_op_4', 'main_net_0_ops_(1, 2)_ops_(1, 2)_op_5', 'main_net_0_ops_(1, 2)_ops_(1, 2)_op_6', 'main_net_0_ops_(1, 2)_ops_(1, 2)_op_7', 'global_avg_pooling', 'classifer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/512 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['pre_processing', 'pre_processing_0', 'pre_processing_1', 'main_net', 'main_net_0', 'main_net_0_ops', 'main_net_0_ops_preproc_x', 'main_net_0_ops_preproc_x_op', 'main_net_0_ops_preproc_x_op_0', 'main_net_0_ops_preproc_x_op_1', 'main_net_0_ops_preproc_x_op_2', 'main_net_0_ops_preproc_x2', 'main_net_0_ops_preproc_x2_op', 'main_net_0_ops_preproc_x2_op_0', 'main_net_0_ops_preproc_x2_op_1', 'main_net_0_ops_preproc_x2_op_2', 'main_net_0_ops_(0, 2)', 'main_net_0_ops_(0, 2)_ops', 'main_net_0_ops_(0, 2)_ops_(0, 1)', 'main_net_0_ops_(0, 2)_ops_(0, 1)_op', 'main_net_0_ops_(0, 2)_ops_(0, 1)_op_0', 'main_net_0_ops_(0, 2)_ops_(0, 1)_op_1', 'main_net_0_ops_(0, 2)_ops_(0, 1)_op_2', 'main_net_0_ops_(0, 2)_ops_(0, 1)_op_3', 'main_net_0_ops_(0, 2)_ops_(0, 1)_op_4', 'main_net_0_ops_(0, 2)_ops_(0, 1)_op_5', 'main_net_0_ops_(0, 2)_ops_(0, 1)_op_6', 'main_net_0_ops_(0, 2)_ops_(0, 1)_op_7', 'main_net_0_ops_(0, 2)_ops_(0, 2)', 'main_net_0_ops_(0, 2)_ops_(0, 2)_op', 'main_net_0_ops_(0, 2)_ops_(0, 2)_op_0', 'main_net_0_ops_(0, 2)_ops_(0, 2)_op_1', 'main_net_0_ops_(0, 2)_ops_(0, 2)_op_2', 'main_net_0_ops_(0, 2)_ops_(0, 2)_op_3', 'main_net_0_ops_(0, 2)_ops_(1, 2)', 'main_net_0_ops_(0, 2)_ops_(1, 2)_op', 'main_net_0_ops_(0, 2)_ops_(1, 2)_op_0', 'main_net_0_ops_(0, 2)_ops_(1, 2)_op_1', 'main_net_0_ops_(0, 2)_ops_(1, 2)_op_2', 'main_net_0_ops_(0, 2)_ops_(1, 2)_op_3', 'main_net_0_ops_(1, 2)', 'main_net_0_ops_(1, 2)_ops', 'main_net_0_ops_(1, 2)_ops_(0, 1)', 'main_net_0_ops_(1, 2)_ops_(0, 1)_op', 'main_net_0_ops_(1, 2)_ops_(0, 2)', 'main_net_0_ops_(1, 2)_ops_(0, 2)_op', 'main_net_0_ops_(1, 2)_ops_(0, 2)_op_0', 'main_net_0_ops_(1, 2)_ops_(0, 2)_op_1', 'main_net_0_ops_(1, 2)_ops_(0, 2)_op_2', 'main_net_0_ops_(1, 2)_ops_(0, 2)_op_3', 'main_net_0_ops_(1, 2)_ops_(0, 2)_op_4', 'main_net_0_ops_(1, 2)_ops_(0, 2)_op_5', 'main_net_0_ops_(1, 2)_ops_(0, 2)_op_6', 'main_net_0_ops_(1, 2)_ops_(0, 2)_op_7', 'main_net_0_ops_(1, 2)_ops_(1, 2)', 'main_net_0_ops_(1, 2)_ops_(1, 2)_op', 'main_net_0_ops_(1, 2)_ops_(1, 2)_op_0', 'main_net_0_ops_(1, 2)_ops_(1, 2)_op_1', 'main_net_0_ops_(1, 2)_ops_(1, 2)_op_2', 'main_net_0_ops_(1, 2)_ops_(1, 2)_op_3', 'main_net_0_ops_(1, 2)_ops_(1, 2)_op_4', 'main_net_0_ops_(1, 2)_ops_(1, 2)_op_5', 'main_net_0_ops_(1, 2)_ops_(1, 2)_op_6', 'main_net_0_ops_(1, 2)_ops_(1, 2)_op_7', 'global_avg_pooling', 'classifer'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/512 [00:10<1:26:58, 10.21s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ec2e0dff8d81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m''' Visualize '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dog.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m _ = render_activation_grid_very_naive(\n\u001b[0m\u001b[1;32m    293\u001b[0m     img, model, cell_image_size=int(args[2]), n_steps=int(args[3]))\n",
      "\u001b[0;32m<ipython-input-12-ec2e0dff8d81>\u001b[0m in \u001b[0;36mrender_activation_grid_very_naive\u001b[0;34m(img, model, layer, cell_image_size, n_steps)\u001b[0m\n\u001b[1;32m    266\u001b[0m         ]\n\u001b[1;32m    267\u001b[0m     )\n\u001b[0;32m--> 268\u001b[0;31m     results = render.render_vis(\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/lucent/optvis/render.py\u001b[0m in \u001b[0;36mrender_vis\u001b[0;34m(model, objective_f, param_f, optimizer, transforms, thresholds, verbose, preprocess, progress, show_image, save_image, image_name, show_inline, fixed_image_size)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_to_img_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/lucent/optvis/render.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m                         )\n\u001b[1;32m    109\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time."
     ]
    }
   ],
   "source": [
    "# Internal Imports\n",
    "from os import error\n",
    "from alpha import Alpha\n",
    "from learnt_model import LearntModel\n",
    "from model import Model\n",
    "from operations import OPS\n",
    "from util import load_alpha\n",
    "\n",
    "# External Imports\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from lucent.modelzoo import inceptionv1, util\n",
    "from lucent.misc.channel_reducer import ChannelReducer\n",
    "from lucent.optvis import objectives, param, render, transform\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "''' Model Setup '''\n",
    "\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Constants\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "# Load model from argument\n",
    "model = torch.load(args[1])\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "print(util.get_model_layers(model))\n",
    "\n",
    "''' Helper Functions'''\n",
    "\n",
    "def parse_layer(layer):\n",
    "    if \"main_net\" in layer: # FIXME: Need to change for HDARTS\n",
    "        _, _, cell_num, _, edge, _ = tuple(layer.split(\"_\"))\n",
    "        return int(cell_num), edge\n",
    "    else:\n",
    "        raise error(\"Invalid Layer Name\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_layer(model, layer, X):\n",
    "    #cell_num, edge = parse_layer(layer)\n",
    "    hook = render.ModuleHook(model.main_net[0].ops[str((1,2))].ops[str((0,1))].op)\n",
    "    model(X)\n",
    "    hook.close()\n",
    "    return f.relu(hook.features)\n",
    "\n",
    "@objectives.wrap_objective()\n",
    "def dot_compare(layer, acts, batch=1):\n",
    "    def inner(T):\n",
    "        pred = T(layer)[batch]\n",
    "        return -(pred * acts).sum(dim=0, keepdims=True).mean()\n",
    "\n",
    "    return inner\n",
    "\n",
    "''' Activation Grid Functions '''\n",
    "\n",
    "def render_activation_grid_less_naive(\n",
    "    img,\n",
    "    model,\n",
    "    layer=\"main_net_0_ops_(1, 2)_ops_(0, 1)_op\",\n",
    "    cell_image_size=60,\n",
    "    n_groups=6,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "):\n",
    "    # First wee need, to normalize and resize the image\n",
    "    img = torch.tensor(np.transpose(img, [2, 0, 1])).to(device)\n",
    "    normalize = (\n",
    "        transform.preprocess_inceptionv1()\n",
    "        if model._get_name() == \"InceptionV1\"\n",
    "        else transform.normalize()\n",
    "    )\n",
    "    transforms = transform.standard_transforms.copy() + [\n",
    "        normalize,\n",
    "        torch.nn.Upsample(size=224, mode=\"bilinear\", align_corners=True),\n",
    "    ]\n",
    "    transforms_f = transform.compose(transforms)\n",
    "    # shape: (1, 3, original height of img, original width of img)\n",
    "    img = img.unsqueeze(0)\n",
    "    # shape: (1, 3, 224, 224)\n",
    "    img = transforms_f(img)\n",
    "\n",
    "    # Here we compute the activations of the layer `layer` using `img` as input\n",
    "    # shape: (layer_channels, layer_height, layer_width), the shape depends on the layer\n",
    "    acts = get_layer(model, layer, img)[0]\n",
    "    # shape: (layer_height, layer_width, layer_channels)\n",
    "    acts = acts.permute(1, 2, 0)\n",
    "    # shape: (layer_height*layer_width, layer_channels)\n",
    "    acts = acts.view(-1, acts.shape[-1])\n",
    "    acts_np = acts.cpu().numpy()\n",
    "    nb_cells = acts.shape[0]\n",
    "\n",
    "    # negative matrix factorization `NMF` is used to reduce the number\n",
    "    # of channels to n_groups. This will be used as the following.\n",
    "    # Each cell image in the grid is decomposed into a sum of\n",
    "    # (n_groups+1) images. First, each cell has its own set of parameters\n",
    "    #  this is what is called `cells_params` (see below). At the same time, we have\n",
    "    # a of group of images of size 'n_groups', which also have their own image parametrized\n",
    "    # by `groups_params`. The resulting image for a given cell in the grid\n",
    "    # is the sum of its own image (parametrized by `cells_params`)\n",
    "    # plus a weighted sum of the images of the group. Each each image from the group\n",
    "    # is weighted by `groups[cell_index, group_idx]`. Basically, this is a way of having\n",
    "    # the possibility to make cells with similar activations have a similar image, because\n",
    "    # cells with similar activations will have a similar weighting for the elements\n",
    "    # of the group.\n",
    "    if n_groups > 0:\n",
    "        reducer = ChannelReducer(n_groups, \"NMF\")\n",
    "        groups = reducer.fit_transform(acts_np)\n",
    "        groups /= groups.max(0)\n",
    "    else:\n",
    "        groups = np.zeros([])\n",
    "    # shape: (layer_height*layer_width, n_groups)\n",
    "    groups = torch.from_numpy(groups)\n",
    "\n",
    "    # Parametrization of the images of the groups (we have 'n_groups' groups)\n",
    "    groups_params, groups_image_f = param.fft_image(\n",
    "        [n_groups, 3, cell_image_size, cell_image_size]\n",
    "    )\n",
    "    # Parametrization of the images of each cell in the grid (we have 'layer_height*layer_width' cells)\n",
    "    cells_params, cells_image_f = param.fft_image(\n",
    "        [nb_cells, 3, cell_image_size, cell_image_size]\n",
    "    )\n",
    "\n",
    "    # First, we need to construct the images of the grid\n",
    "    # from the parameterizations\n",
    "\n",
    "    def image_f():\n",
    "        groups_images = groups_image_f()\n",
    "        cells_images = cells_image_f()\n",
    "        X = []\n",
    "        for i in range(nb_cells):\n",
    "            x = 0.7 * cells_images[i] + 0.5 * sum(\n",
    "                groups[i, j] * groups_images[j] for j in range(n_groups)\n",
    "            )\n",
    "            X.append(x)\n",
    "        X = torch.stack(X)\n",
    "        return X\n",
    "\n",
    "    # make sure the images are between 0 and 1\n",
    "    image_f = param.to_valid_rgb(image_f, decorrelate=True)\n",
    "\n",
    "    # After constructing the cells images, we sample randomly a mini-batch of cells\n",
    "    # from the grid. This is to prevent memory overflow, especially if the grid\n",
    "    # is large.\n",
    "    def sample(image_f, batch_size):\n",
    "        def f():\n",
    "            X = image_f()\n",
    "            inds = torch.randint(0, len(X), size=(batch_size,))\n",
    "            inputs = X[inds]\n",
    "            # HACK to store indices of the mini-batch, because we need them\n",
    "            # in objective func. Might be better ways to do that\n",
    "            sample.inds = inds\n",
    "            return inputs\n",
    "\n",
    "        return f\n",
    "\n",
    "    image_f_sampled = sample(image_f, batch_size=batch_size)\n",
    "\n",
    "    # Now, we define the objective function\n",
    "\n",
    "    def objective_func(model):\n",
    "        # shape: (batch_size, layer_channels, cell_layer_height, cell_layer_width)\n",
    "        pred = f.relu(model(layer))\n",
    "        # use the sampled indices from `sample` to get the corresponding targets\n",
    "        target = acts[sample.inds].to(pred.device)\n",
    "        # shape: (batch_size, layer_channels, 1, 1)\n",
    "        target = target.view(target.shape[0], target.shape[1], 1, 1)\n",
    "        dot = (pred * target).sum(dim=1).mean()\n",
    "        return -dot\n",
    "\n",
    "    obj = objectives.Objective(objective_func)\n",
    "\n",
    "    def param_f():\n",
    "        # We optimize the parametrizations of both the groups and the cells\n",
    "        params = list(groups_params) + list(cells_params)\n",
    "        return params, image_f_sampled\n",
    "\n",
    "    results = render.render_vis(\n",
    "        model,\n",
    "        obj,\n",
    "        param_f,\n",
    "        thresholds=(n_steps,),\n",
    "        show_image=False,\n",
    "        progress=True,\n",
    "        fixed_image_size=cell_image_size,\n",
    "    )\n",
    "    # shape: (layer_height*layer_width, 3, grid_image_size, grid_image_size)\n",
    "    imgs = image_f()\n",
    "    imgs = imgs.cpu().data\n",
    "    imgs = imgs[:, :, 2:-2, 2:-2]\n",
    "    # turn imgs into a a grid\n",
    "    grid = torchvision.utils.make_grid(imgs, nrow=int(np.sqrt(nb_cells)), padding=0)\n",
    "    grid = grid.permute(1, 2, 0)\n",
    "    grid = grid.numpy()\n",
    "    render.show(grid)\n",
    "    return imgs\n",
    "\n",
    "def render_activation_grid_very_naive(\n",
    "    img, model, layer=\"main_net_0_ops_(1, 2)_ops_(0, 1)_op\", cell_image_size=48, n_steps=1024\n",
    "):\n",
    "    # First wee need, to normalize and resize the image\n",
    "    img = torch.tensor(np.transpose(img, [2, 0, 1])).to(device)\n",
    "    normalize = (\n",
    "        transform.preprocess_inceptionv1()\n",
    "        if model._get_name() == \"InceptionV1\"\n",
    "        else transform.normalize()\n",
    "    )\n",
    "    transforms = [\n",
    "        normalize,\n",
    "        torch.nn.Upsample(size=224, mode=\"bilinear\", align_corners=True),\n",
    "    ]\n",
    "    transforms_f = transform.compose(transforms)\n",
    "    # shape: (1, 3, original height of img, original width of img)\n",
    "    img = img.unsqueeze(0)\n",
    "    # shape: (1, 3, 224, 224)\n",
    "    img = transforms_f(img)\n",
    "\n",
    "    # Here we compute the activations of the layer `layer` using `img` as input\n",
    "    # shape: (layer_channels, layer_height, layer_width), the shape depends on the layer\n",
    "    acts = get_layer(model, layer, img)[0]\n",
    "    layer_channels, layer_height, layer_width = acts.shape\n",
    "    # for each position `(y, x)` in the feature map `acts`, we optimize an image\n",
    "    # to match with the features `acts[:, y, x]`\n",
    "    # This means that the total number of cells (which is the batch size here) \n",
    "    # in the grid is layer_height*layer_width.\n",
    "    nb_cells = layer_height * layer_width\n",
    "\n",
    "    # Parametrization of the of each cell in the grid\n",
    "    param_f = lambda: param.image(\n",
    "        cell_image_size, batch=nb_cells\n",
    "    )\n",
    "    params, image_f = param_f()\n",
    "\n",
    "    obj = objectives.Objective.sum(\n",
    "        [\n",
    "            # for each position in `acts`, maximize the dot product between the activations\n",
    "            # `acts` at the position (y, x) and the features of the corresponding\n",
    "            # cell image on our 'grid'. The activations at (y, x) is a vector of size\n",
    "            # `layer_channels` (this depends on the `layer`). The features\n",
    "            # of the corresponding cell on our grid is a tensor of shape\n",
    "            # (layer_channels, cell_layer_height, cell_layer_width).\n",
    "            # Note that cell_layer_width != layer_width and cell_layer_height != layer_weight\n",
    "            # because the cell image size is smaller than the image size.\n",
    "            # With `dot_compare`, we maximize the dot product between\n",
    "            # cell_activations[y_cell, x_xcell] and acts[y,x] (both of size `layer_channels`)\n",
    "            # for each possible y_cell and x_cell, then take the average to get a single\n",
    "            # number. Check `dot_compare for more details.`\n",
    "            dot_compare(layer, acts[:, y:y+1, x:x+1], batch=x + y * layer_width)\n",
    "            for i, (x, y) in enumerate(product(range(layer_width), range(layer_height)))\n",
    "        ]\n",
    "    )\n",
    "    results = render.render_vis(\n",
    "        model,\n",
    "        obj,\n",
    "        param_f,\n",
    "        thresholds=(n_steps,),\n",
    "        progress=True,\n",
    "        fixed_image_size=cell_image_size,\n",
    "        show_image=False,\n",
    "    )\n",
    "    # shape: (layer_height*layer_width, cell_image_size, cell_image_size, 3)\n",
    "    imgs = results[-1] # last step results\n",
    "    # shape: (layer_height*layer_width, 3, cell_image_size, cell_image_size)\n",
    "    imgs = imgs.transpose((0, 3, 1, 2))\n",
    "    imgs = torch.from_numpy(imgs)\n",
    "    imgs = imgs[:, :, 2:-2, 2:-2]\n",
    "    # turn imgs into a a grid\n",
    "    grid = torchvision.utils.make_grid(imgs, nrow=int(np.sqrt(nb_cells)), padding=0)\n",
    "    grid = grid.permute(1, 2, 0)\n",
    "    grid = grid.numpy()\n",
    "    render.show(grid)\n",
    "    return imgs\n",
    "\n",
    "''' Visualize '''\n",
    "img = np.array(Image.open(\"dog.jpg\"), np.float32)\n",
    "_ = render_activation_grid_very_naive(\n",
    "    img, model, cell_image_size=int(args[2]), n_steps=int(args[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
